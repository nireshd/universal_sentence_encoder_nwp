# universal_sentence_encoder_nwp
A simple next word predictor using Google's Universal Sentence Encoder. 

## Files:
1. doc3.py: a short corpus used for training.
2. use_embedding.py: the script for training a small LSTM model, with the Universal Sentence Encoder as the embedding layer.
3. word_embedding.h5: a saved trained model.
4. tokenizer_saved.pkl: a pickle file of the nltk tokenizer used.
5. use_next_word_prediction.ipynb: A jupyter notebook linked to colab to demonstrate the text predictor. 
